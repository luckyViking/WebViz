<!DOCTYPE html>
<html>
<head>
    <title>Support Vector Machines</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <script async>
        $(document).ready(() => {
            import("../js/svm.js").then(svm => IrisShapesPlotly(svm, $('#datasets').val(), $('#kernel').val(), $('#epsilon').val()));

            $('#forwards').click(function (e) {
                e.preventDefault();
                console.log("Forward");
                var element = $('.explanation.active');
                element.fadeOut(1000);
                element.removeClass('active');
                element.next().fadeIn(3000);
                element.next().addClass('active');
            });

            $('#reload').click(function (e) {
                e.preventDefault();
                console.log("Reload");

                var active = $('.explanation.active');
                active.fadeOut(1000);
                active.removeClass('active');

                var first = $('.explanation').first();
                first.fadeIn(3000);
                first.addClass('active');

                import("../js/svm.js").then(svm => IrisShapesPlotly(svm, $('#datasets').val(), $('#kernel').val(), $('#epsilon').val()));
            });

        })
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
    <script>
        MathJax = {
            tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]},
            svg: {fontCache: 'global'}
        };
    </script>
    <link rel="stylesheet" type="text/css" href="../css/styles.css">
</head>
<body>
<div class="col-lg-12" style="height:30px; text-align: left;">
    <a href="index.html" class="previous">&laquo; Previous</a>
</div>

<div class="jumbotron text-center">
    <h1>Support Vector machines</h1>
    <p>A Visualization</p>
</div>

<div class="container">
    <div class="row">
        <div class="">
            <h2>Introduction</h2>
            <p>This website was created as part of the digitalization project "Webtools for teaching" at <a
                    href="https://www.frankfurt-university.de/en/" target="_blank">Frankfurt University of Applied
                Sciences (FRA UAS)</a>. "Computer-based visualization systems provide visual representations of datasets
                designed to help people carry out tasks more effectively." External representation replaces cognition
                with perception. We hope that our interactive visualization will make the SVM easier to understand for
                you.</p>
        </div>
    </div>
    <div class="row">
        <div class="col-md-12 col-lg-8 text-center">
            <div class="scatter" style="height:500px; width:100%;" id="iris-shapes"></div>
        </div>
        <div class="col-md-12 col-lg-4">
            <form class="form-horizontal" action="">
                <h3>Settings</h3>
                <div class="form-group">
                    <label for="datasets" class="control-label col-sm-2">Dataset</label>
                    <div class="col-sm-10">
                        <select id="datasets" name="datasets" class="form-control input-sm">
                            <option value="iris">Iris</option>
                            <option>...</option>
                        </select>
                    </div>
                </div>
                <div class="form-group">
                    <label for="kernel" class="control-label col-sm-2">Kernel</label>
                    <div class="col-sm-10">
                        <select id="kernel" name="kernel" class="form-control input-sm ">
                            <option value="linear">Linear</option>
                            <option value="polynomial">Polynomial</option>
                        </select>
                    </div>
                </div>
                <div class="form-group">
                    <label for="epsilon" class="control-label col-sm-2">Epsilon</label>
                    <div class="col-sm-10">
                        <input type="number" name="epsilon" id="epsilon" value="1e-3" class="form-control input-sm"/>
                    </div>
                </div>
                <div class="form-group">
                    <div class="col-sm-6">
                        <button type="button" id="reload" class="form-control btn btn-success">Reload</button>
                    </div>
                    <div class=" col-sm-6">
                        <button type="button" id="forwards" class="form-control btn btn-primary">Step forwards &rarr;
                        </button>
                    </div>
                </div>
            </form>
        </div>
    </div>

    <div class="row">
        <!--
        <ol class="timeline">
            <li>
                <p class="timelineText">Why?</p>
                <span class="point"></span>
            </li>
            <li>
                <p class="timelineText">How?</p>
                <span class="point"></span>
            </li>
            <li>
                <p class="timelineText">...</p>
                <span class="point"></span>
            </li>
            <li>
                <p class="timelineText">...</p>
                <span class="point"></span>
            <li>
                <p class="timelineText">References</p>
                <span class="point"></span>
            </li>
        </ol>-->
    </div>

    <div class="row">
        <div class="col-md-12 col-lg-12">
            <div class="explanation active">
                <h2>Why SVM?</h2>
                The Support Vector Machine (SVM) is one of the most modern technique used for regression-and
                classification problems.
                The SVM is a supervised learning model.
                In this approach we are given a set of input vectors ($x_n$) paired with corresponding target values
                ($t_n$).
                The goal is to "learn" a model made out of these training set to make correct predictions of t for newly
                presented input data $x$.
                In classification, $t$ is discrete and represented as class labels.
                In regression $t$ is a continuous variable. The prediction $y(x,w)$ is expressed as the following linear
                model:
                $$y(x,w) = \sum_{m=0}^{M} w_m \phi_m(x) = w^T\phi \label{ref1}$$
                Here, $\phi_{m}(x)$ are basis functions and the parameters ${w_m}$ are the associated weights.
            </div>
            <div class="explanation">
                <h2>How SVM works</h2>
                There are different types of the SVM: in the classification case, there is the original one the Maximal
                Margin Classifier, the kernelized version, the soft-margin version and the soft-margin kernelized
                version which combines the first 3 versions and is used most frequently.
                In the regression case there is the Support Vector Regression (SVR).
                However, all SVMs are basically a specialization of (\ref{ref1}) and make predictions based on the
                following function:
                $$y(x,w)=\sum_{i=1}^{N}w_{i}K(x,x_i)+w_0 \label{ref2}$$
                where K stands for kernel functions.
            </div>
            <div class="explanation">
                The SVM searches for the optimal hyperplane that best separates the data by maximizing the margin.
                Maximizing the margin is an optimization problem that is equivalent to minimizing the norm of the weight
                vector:
                $$minimize_{w,b} \space \frac{1} {2} ||w||^2 $$
                subject to $$y_{i}(w * x_{i} + b)-1 \geq 0, \space i=1,...,m \label{ref3}$$
                The convex quadratic optimization problem from (\ref{ref3}) can be solved by lagrange multipliers:
                $$\mathcal{L}(w,b,\alpha) = \mathcal{f}(w)-\sum_{i=1}^{m}\alpha_{i}g_{i}(w,b)$$
                $$\mathcal{L}(w,b,\alpha) = \frac{1}{2}||w||^{2}-\sum_{i=1}^{m}\alpha_{i}[y_{i}(w*x_{i}+b)-1]$$
                $$\min_{w,b} \space \max_{a} \mathcal{L}(w,b,\alpha)$$
                subject to
                $$\alpha_{i}\geq0, \space i=1,...,m \label{ref4}$$
                The problem from (\ref{ref4}) can then be rewritten in dual form and becomes a Wolfe Dual Problem.
                In this way the parameters $w$ and $b$ can be found:
                $$w=\sum_{i=1}^{m}\alpha_{i}y_{i}x_{i} \label{ref5}$$
                $$b=y_{i}-w*x_{i} \label{ref6}$$
            </div>
            <div class="explanation">
                Because (\ref{ref4}) contains inequality constraints, the solution must fulfill certain conditions, the
                Karush-Kuhn-Tucker (KKT) conditions.
                These conditions are: stationarity-, primal feasibility-, dual feasibility- and complementary slackness
                condition.
                $$\alpha_{i}[y_{i}(w*x_{i}+b)-1]=0 \space for \space for \space all \space i=1,...,m \label{ref7}$$
                The fourth one, the complementary slackness condition from (\ref{ref7}), says either $\alpha_{i}$ or the
                original condition should be zero.
                <strong>The support vectors are the ones that have a positive $\alpha_{i}$ which means the second
                    condition is active.</strong>
                If a solution meets the four KKT conditions, this solution is optimal.
                However, unclean data, for instance outliers, are not uncommon and impair linear separability.
                Vapnik and Cortes modified the original SVM to the <strong>Soft Margin SVM</strong> which means, in the
                classification case, <strong>outliers are allowed</strong>.
                The original optimization problem from (\ref{ref3}) has been changed to (\label{ref8}):
                the slack variable $\zeta$ and the hyperparameter $C$ have been added.
                Regarding to this, the outliers can now be on the "wrong side" but a hyperplane can still be found.
                $$minimize_w,b,\zeta \space \frac{1}{2}||w||^{2} + C \sum_{i=1}^{m} \zeta_{i} \label{ref8}$$
                subject to
                $$y_{i}(w*x_{i}+b) \geq 1-\zeta_{i}$$
                $$\zeta_{i} \geq 0 \space for \space any \space i=1,...,m $$
                The hyperparameter $C$ can then be used to control how the SVM should deal with outliers.
            </div>
            <div class="explanation">
                <h2>References</h2>...
                <p></p>
            </div>

        </div>
    </div>

    <hr>
    <footer>
        <span>Frankfurt University of Applied Sciences</span>
        <br>
        <span>WebTools f√ºr die Lehre: SVM</span>
        <br>
        <br>
        <span>Contributors:</span>
        <br>
        <span>Yasemin Er; </span>
        <span>Hendrik Pfaff; </span>
        <span>Lukas Atkinson; </span>
        <span>Luca Jordan</span>

    </footer>

    <script src="../js/plotly_gui.js"></script>
</div>
</body>
</html>
